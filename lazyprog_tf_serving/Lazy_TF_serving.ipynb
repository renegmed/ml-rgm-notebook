{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-731833107751/mnist-data\n"
     ]
    }
   ],
   "source": [
    "training_data_uri = 's3://sagemaker-us-east-1-731833107751/mnist-data'\n",
    "print(training_data_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amplify-my-app-dev-204117-deployment\n",
      "amplify-rnamplify-dev-150850-deployment\n",
      "amplify-rnamplifyweek-testing-111251-deployment\n",
      "astoria-event-dev-serverlessdeploymentbucket-1txdvzi3u1fo6\n",
      "elasticbeanstalk-us-east-1-731833107751\n",
      "renegmed-aicamp\n",
      "renegmed-sagemaker-container-webinar\n",
      "sagemaker-us-east-1-731833107751\n"
     ]
    }
   ],
   "source": [
    "# Store in S3\n",
    "\n",
    "# s3 = boto3.resource(\"s3\")\n",
    "# for bucket in s3.buckets.all():\n",
    "#     print(bucket.name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "np.save('/tmp/x_train', x_train)\n",
    "np.save('/tmp/y_train', y_train)\n",
    "np.save('/tmp/x_test', x_test)\n",
    "np.save('/tmp/y_test', y_test)\n",
    "#s3.Bucket('sagemaker-us-east-1-731833107751').upload_file('ec2_file', 's3_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.Bucket('sagemaker-us-east-1-731833107751').upload_file('/tmp/x_train.npy', 'mnist-data/x_train.npy') \n",
    "s3.Bucket('sagemaker-us-east-1-731833107751').upload_file('/tmp/y_train.npy', 'mnist-data/y_train.npy')\n",
    "s3.Bucket('sagemaker-us-east-1-731833107751').upload_file('/tmp/x_test.npy', 'mnist-data/x_test.npy')\n",
    "s3.Bucket('sagemaker-us-east-1-731833107751').upload_file('/tmp/y_test.npy', 'mnist-data/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#import matplotlib.pyplot as plt \u001b[39;49;00m\n",
      "\u001b[37m#import subprocess\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.keras.layers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Input, Conv2D, Dense, Flatten, Dropout\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.keras.models\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Model\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel\u001b[39;49;00m(x_train, y_train, x_test, y_test):\n",
      "    \u001b[33m\"\"\"Generate a simple model\"\"\"\u001b[39;49;00m\n",
      "\u001b[37m#     model = tf.keras.models.Sequential([\u001b[39;49;00m\n",
      "\u001b[37m#         tf.keras.layers.Flatten(),\u001b[39;49;00m\n",
      "\u001b[37m#         tf.keras.layers.Dense(1024, activation=tf.nn.relu),\u001b[39;49;00m\n",
      "\u001b[37m#         tf.keras.layers.Dropout(0.4),\u001b[39;49;00m\n",
      "\u001b[37m#         tf.keras.layers.Dense(10, activation=tf.nn.softmax)\u001b[39;49;00m\n",
      "\u001b[37m#     ])\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     model.compile(optimizer='adam',\u001b[39;49;00m\n",
      "\u001b[37m#                   loss='sparse_categorical_crossentropy',\u001b[39;49;00m\n",
      "\u001b[37m#                   metrics=['accuracy'])\u001b[39;49;00m\n",
      "\u001b[37m#     model.fit(x_train, y_train)\u001b[39;49;00m\n",
      "\u001b[37m#     model.evaluate(x_test, y_test)\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     return model\u001b[39;49;00m\n",
      "\n",
      "    K = \u001b[36mlen\u001b[39;49;00m(\u001b[36mset\u001b[39;49;00m(y_train))\n",
      "    \u001b[37m# Build the model using the functional API\u001b[39;49;00m\n",
      "    i = Input(shape=x_train[\u001b[34m0\u001b[39;49;00m].shape)  \u001b[37m# tensorflow.keras.layers.Input\u001b[39;49;00m\n",
      "    x = Conv2D(\u001b[34m32\u001b[39;49;00m, (\u001b[34m3\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m), strides=\u001b[34m2\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(i)\n",
      "    x = Conv2D(\u001b[34m64\u001b[39;49;00m, (\u001b[34m3\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m), strides=\u001b[34m2\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(x)\n",
      "    x = Conv2D(\u001b[34m128\u001b[39;49;00m, (\u001b[34m3\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m), strides=\u001b[34m2\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(x)\n",
      "    x = Flatten()(x)\n",
      "    x = Dropout(\u001b[34m0.2\u001b[39;49;00m)(x)\n",
      "    x = Dense(\u001b[34m512\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(x)\n",
      "    x = Dropout(\u001b[34m0.2\u001b[39;49;00m)(x)\n",
      "    x = Dense(K, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(x)\n",
      "\n",
      "    model = Model(i, x)\n",
      "\n",
      "    model.compile(optimizer=\u001b[33m'\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "             loss=\u001b[33m'\u001b[39;49;00m\u001b[33msparse_categorical_crossentropy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "             metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=\u001b[34m5\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_load_data\u001b[39;49;00m(base_dir):\n",
      "    \u001b[37m# Load in the data\u001b[39;49;00m\n",
      "\u001b[37m#     fashion_mnist = tf.keras.datasets.fashion_mnist\u001b[39;49;00m\n",
      "\u001b[37m#     (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\u001b[39;49;00m\n",
      "    x_train = np.load(os.path.join(base_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mx_train.npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    y_train = np.load(os.path.join(base_dir, \u001b[33m'\u001b[39;49;00m\u001b[33my_train.npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    x_test = np.load(os.path.join(base_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mx_test.npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    y_test = np.load(os.path.join(base_dir, \u001b[33m'\u001b[39;49;00m\u001b[33my_test.npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    \n",
      "    x_train, x_test = x_train / \u001b[34m255.0\u001b[39;49;00m, x_test/ \u001b[34m255.0\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# the data is only 2D!\u001b[39;49;00m\n",
      "    \u001b[37m# convolution expects height x width x color\u001b[39;49;00m\n",
      "    x_train = np.expand_dims(x_train, -\u001b[34m1\u001b[39;49;00m)\n",
      "    x_test = np.expand_dims(x_test, -\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m x_train, y_train, x_test, y_test\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_parse_args\u001b[39;49;00m():\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\n",
      "    \u001b[37m# model_dir is always passed in from SageMaker. By default this is a S3 path under the default bucket.\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--sm-model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_known_args()\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    args, unknown = _parse_args()\n",
      "\n",
      "    train_data, train_labels, eval_data, eval_labels = _load_data(args.train)\n",
      "\u001b[37m#     eval_data, eval_labels = _load_testing_data(args.train)\u001b[39;49;00m\n",
      "\n",
      "    mnist_classifier = model(train_data, train_labels, eval_data, eval_labels)\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m args.current_host == args.hosts[\u001b[34m0\u001b[39;49;00m]:\n",
      "        \u001b[37m# save model to an S3 directory with version number '00000001'\u001b[39;49;00m\n",
      "        mnist_classifier.save(os.path.join(args.sm_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33m000000002\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mmnist_model.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow 2.0 script\n",
    "!pygmentize 'mnist-2.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "mnist_estimator2 = TensorFlow(entry_point='mnist-2.py',\n",
    "                             role=role,\n",
    "                             train_instance_count=1,\n",
    "                             train_instance_type='local',\n",
    "                             #train_instance_type='ml.m5.4xlarge',\n",
    "                             framework_version='2.0.0',\n",
    "                             py_version='py3',\n",
    "                             distributions={'parameter_server': {'enabled': True}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tmp53_xy1tq_algo-1-pqh24_1 ... \n",
      "\u001b[1BAttaching to tmp53_xy1tq_algo-1-pqh24_12mdone\u001b[0m\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m 2020-01-31 21:32:09,426 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m 2020-01-31 21:32:09,434 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m 2020-01-31 21:32:09,652 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m 2020-01-31 21:32:09,674 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m 2020-01-31 21:32:09,693 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m 2020-01-31 21:32:09,705 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m \n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m Training Env:\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m \n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m {\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"additional_framework_parameters\": {\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m         \"sagemaker_parameter_server_enabled\": true\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m         \"training\": \"/opt/ml/input/data/training\"\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"current_host\": \"algo-1-pqh24\",\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"hosts\": [\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m         \"algo-1-pqh24\"\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     ],\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m         \"model_dir\": \"s3://sagemaker-us-east-1-731833107751/tensorflow-training-2020-01-31-21-32-03-695/model\"\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m         \"training\": {\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m         }\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"job_name\": \"tensorflow-training-2020-01-31-21-32-03-695\",\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"master_hostname\": \"algo-1-pqh24\",\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-731833107751/tensorflow-training-2020-01-31-21-32-03-695/source/sourcedir.tar.gz\",\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"module_name\": \"mnist-2\",\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"num_cpus\": 2,\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m         \"current_host\": \"algo-1-pqh24\",\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m         \"hosts\": [\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m             \"algo-1-pqh24\"\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m         ]\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m     \"user_entry_point\": \"mnist-2.py\"\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m \n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m Environment variables:\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m \n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_HOSTS=[\"algo-1-pqh24\"]\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_HPS={\"model_dir\":\"s3://sagemaker-us-east-1-731833107751/tensorflow-training-2020-01-31-21-32-03-695/model\"}\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_USER_ENTRY_POINT=mnist-2.py\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_FRAMEWORK_PARAMS={\"sagemaker_parameter_server_enabled\":true}\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-pqh24\",\"hosts\":[\"algo-1-pqh24\"]}\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_INPUT_DATA_CONFIG={\"training\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_CHANNELS=[\"training\"]\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_CURRENT_HOST=algo-1-pqh24\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_MODULE_NAME=mnist-2\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_NUM_CPUS=2\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-731833107751/tensorflow-training-2020-01-31-21-32-03-695/source/sourcedir.tar.gz\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_parameter_server_enabled\":true},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-pqh24\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1-pqh24\"],\"hyperparameters\":{\"model_dir\":\"s3://sagemaker-us-east-1-731833107751/tensorflow-training-2020-01-31-21-32-03-695/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tensorflow-training-2020-01-31-21-32-03-695\",\"log_level\":20,\"master_hostname\":\"algo-1-pqh24\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-731833107751/tensorflow-training-2020-01-31-21-32-03-695/source/sourcedir.tar.gz\",\"module_name\":\"mnist-2\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-pqh24\",\"hosts\":[\"algo-1-pqh24\"]},\"user_entry_point\":\"mnist-2.py\"}\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_USER_ARGS=[\"--model_dir\",\"s3://sagemaker-us-east-1-731833107751/tensorflow-training-2020-01-31-21-32-03-695/model\"]\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m SM_HP_MODEL_DIR=s3://sagemaker-us-east-1-731833107751/tensorflow-training-2020-01-31-21-32-03-695/model\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m \n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m \n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m /usr/bin/python3 mnist-2.py --model_dir s3://sagemaker-us-east-1-731833107751/tensorflow-training-2020-01-31-21-32-03-695/model\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m \n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m \n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m Train on 60000 samples, validate on 10000 samples\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m Epoch 1/5\n",
      "60000/60000 [==============================] - 19s 322us/sample - loss: 0.5218 - accuracy: 0.8066 - val_loss: 0.3976 - val_accuracy: 0.8542\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m Epoch 2/5\n",
      "60000/60000 [==============================] - 18s 306us/sample - loss: 0.3561 - accuracy: 0.8673 - val_loss: 0.3405 - val_accuracy: 0.8710\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m Epoch 3/5\n",
      "60000/60000 [==============================] - 18s 306us/sample - loss: 0.3081 - accuracy: 0.8844 - val_loss: 0.3137 - val_accuracy: 0.8845\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m Epoch 4/5\n",
      "60000/60000 [==============================] - 18s 304us/sample - loss: 0.2760 - accuracy: 0.8962 - val_loss: 0.3248 - val_accuracy: 0.8787\n",
      "\u001b[36malgo-1-pqh24_1  |\u001b[0m Epoch 5/5\n"
     ]
    }
   ],
   "source": [
    "mnist_estimator2.fit(training_data_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor2 = mnist_estimator2.deploy(initial_instance_count=1, instance_type='ml.c5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model to a temporary directory\n",
    "# import tempfile\n",
    "\n",
    "# MODEL_DIR = tempfile.gettempdir()\n",
    "# version = 1\n",
    "# export_path = os.path.join(MODEL_DIR, str(version))\n",
    "\n",
    "# print('export_path = {}\\n'.format(export_path))\n",
    "# if os.path.isdir(export_path):\n",
    "#     print('\\nAlready saved a model, cleaning up\\n')\n",
    "#     !rm -r {export_path}\n",
    "    \n",
    "# tf.saved_model.save(model, export_path)\n",
    "# # model.save(export_path)\n",
    "\n",
    "# print('\\nSaved model:')\n",
    "# !ls -l {export_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.saved_model.contains_saved_model(export_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!saved_model_cli show --dir {export_path} --all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_model = tf.saved_model.load(export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs = new_model(x_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = new_model.signatures[\"serving_default\"]\n",
    "#print(f(x=tf.constant([[1.]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
